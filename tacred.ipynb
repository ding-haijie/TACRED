{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f90694d7-d5fa-4648-a36b-33c0774d0291",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252aa897-9b70-4c7d-8df3-92b87607ba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import os\n",
    "import math\n",
    "import logging\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "import torch\n",
    "from torch import optim, nn, Tensor\n",
    "\n",
    "root_dir = '.'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3469ac-05ac-4643-b3cb-a4e1502b3df8",
   "metadata": {},
   "source": [
    "# constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b35a33-3ecf-4e3b-8a90-f4eb4ed774fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 100\n",
    "\n",
    "# relation dict\n",
    "rel2id = {'per:title': 0, 'per:stateorprovinces_of_residence': 1,\n",
    "          'per:stateorprovince_of_death': 2, 'per:stateorprovince_of_birth': 3,\n",
    "          'per:spouse': 4, 'per:siblings': 5, 'per:schools_attended': 6,\n",
    "          'per:religion': 7, 'per:parents': 8, 'per:other_family': 9,\n",
    "          'per:origin': 10, 'per:employee_of': 11, 'per:date_of_death': 12,\n",
    "          'per:date_of_birth': 13, 'per:country_of_death': 14, 'per:country_of_birth': 15,\n",
    "          'per:countries_of_residence': 16, 'per:city_of_death': 17, 'per:city_of_birth': 18,\n",
    "          'per:cities_of_residence': 19, 'per:children': 20, 'per:charges': 21,\n",
    "          'per:cause_of_death': 22, 'per:alternate_names': 23, 'per:age': 24,\n",
    "          'org:website': 25, 'org:top_members/employees': 26, 'org:subsidiaries': 27,\n",
    "          'org:stateorprovince_of_headquarters': 28, 'org:shareholders': 29,\n",
    "          'org:political/religious_affiliation': 30, 'org:parents': 31,\n",
    "          'org:number_of_employees/members': 32, 'org:members': 33,\n",
    "          'org:member_of': 34, 'org:founded_by': 35, 'org:founded': 36,\n",
    "          'org:dissolved': 37, 'org:country_of_headquarters': 38,\n",
    "          'org:city_of_headquarters': 39, 'org:alternate_names': 40,\n",
    "          'no_relation': 41}\n",
    "id2rel = {key: value for value, key in rel2id.items()}\n",
    "\n",
    "# part-of-speech dict\n",
    "pos2id = {'PAD': 0, 'UNK': 1, 'NNP': 2, 'NN': 3, 'IN': 4, 'DT': 5, ',': 6, 'JJ': 7, 'NNS': 8, 'VBD': 9, 'CD': 10,\n",
    "          'CC': 11, '.': 12, 'RB': 13, 'VBN': 14, 'PRP': 15, 'TO': 16, 'VB': 17, 'VBG': 18, 'VBZ': 19, 'PRP$': 20,\n",
    "          ':': 21, 'POS': 22, '\\'\\'': 23, '``': 24, '-RRB-': 25, '-LRB-': 26, 'VBP': 27, 'MD': 28, 'NNPS': 29, 'WP': 30,\n",
    "          'WDT': 31, 'WRB': 32, 'RP': 33, 'JJR': 34, 'JJS': 35, '$': 36, 'FW': 37, 'RBR': 38, 'SYM': 39, 'EX': 40,\n",
    "          'RBS': 41, 'WP$': 42, 'PDT': 43, 'LS': 44, 'UH': 45, '#': 46}\n",
    "\n",
    "# named-entity-recognition dict\n",
    "ner2id = {'PAD': 0, 'UNK': 1, 'O': 2, 'PERSON': 3, 'ORGANIZATION': 4, 'LOCATION': 5, 'DATE': 6, 'NUMBER': 7,\n",
    "          'MISC': 8, 'DURATION': 9, 'MONEY': 10, 'PERCENT': 11, 'ORDINAL': 12, 'TIME': 13, 'SET': 14}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf93482-392b-4c4c-98ad-a21bfea224f6",
   "metadata": {},
   "source": [
    "# args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc3c4f0-c722-4560-8192-057c7ea5dc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# batch_size\n",
    "parser.add_argument('--batch_size', default=32,\n",
    "                    type=int, help='Size of mini batch.')\n",
    "# max_epochs\n",
    "parser.add_argument('--max_epochs', default=30, type=int,\n",
    "                    help='Max epochs for training.')\n",
    "# use_glove\n",
    "parser.add_argument('--use_glove', default=False, type=bool,\n",
    "                    help='Whether to use Glove as Pre_trained Embedding')\n",
    "# word_embed_dim\n",
    "parser.add_argument('--word_embed_dim', default=300, type=int,\n",
    "                    help='Size of Word Embedding.')\n",
    "# part-of-speech embed_dim\n",
    "parser.add_argument('--pos_embed_dim', default=30, type=int,\n",
    "                    help='Size of Part-of-Speech Embedding.')\n",
    "# ner_embed_dim\n",
    "parser.add_argument('--ner_embed_dim', default=30, type=int,\n",
    "                    help='Size of Named-Entity-Recognition Embedding.')\n",
    "# position_embed_dim\n",
    "parser.add_argument('--position_embed_dim', default=30, type=int,\n",
    "                    help='Size of Position Encoding Embedding.')\n",
    "# hidden_dim\n",
    "parser.add_argument('--hidden_dim', default=300, type=int,\n",
    "                    help='Size of Hidden Layer.')\n",
    "# attn_dim\n",
    "parser.add_argument('--attn_dim', default=300, type=int,\n",
    "                    help='Size of Attention Layer.')\n",
    "# dropout\n",
    "parser.add_argument('--dropout', default=0.5, type=float,\n",
    "                    help='Dropout rate.')\n",
    "# optimizer\n",
    "parser.add_argument('--optimizer', default='sgd', type=str,\n",
    "                    help='Choose Optimizer from SGD/ Adam/ Adadelta.')\n",
    "# learning rate\n",
    "parser.add_argument('--lr', default=1e-1, type=float,\n",
    "                    help='Initial learning rate.')\n",
    "# momentum\n",
    "parser.add_argument('--momentum', default=0.9, type=float,\n",
    "                    help='Momentum factor for SGD.')\n",
    "# weight_decay\n",
    "parser.add_argument('--weight_decay', default=1e-2, type=float,\n",
    "                    help='Weight decay (L2 penalty).')\n",
    "# grad_clip\n",
    "parser.add_argument('--grad_clip', default=5.0, type=int,\n",
    "                    help='Max norm of the gradients clipping.')\n",
    "# random_seed\n",
    "parser.add_argument('--random_seed', default=16, type=int,\n",
    "                    help='Sets the seed for generating random numbers.')\n",
    "# resume\n",
    "parser.add_argument('--resume', default=False, type=bool,\n",
    "                    help='Whether to load the checkpoints to resume training.')\n",
    "\n",
    "args = parser.parse_args(args=[])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64b8517-21ae-4269-9f1d-da6b091015d0",
   "metadata": {},
   "source": [
    "# data_processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da94c648-4e01-4203-80b5-eb997d82b43a",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5515b632-7606-4e7f-9303-3dd80662b57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    def counter(sequences, threshold):\n",
    "        words_cnt = dict()\n",
    "        for seq in sequences:\n",
    "            for token in seq:\n",
    "                if token in words_cnt:\n",
    "                    words_cnt[token] += 1\n",
    "                else:\n",
    "                    words_cnt[token] = 1\n",
    "        words_cnt = {key: words_cnt[key]\n",
    "                     for key in words_cnt if words_cnt[key] >= threshold}\n",
    "        words_cnt = sorted(words_cnt.items(), key=lambda x: x[1], reverse=True)\n",
    "        return list(map(lambda x: x[0], words_cnt))\n",
    "\n",
    "    def entity_masks(_samp):\n",
    "        _t = [token.lower() for token in _samp['token']]\n",
    "        subj_start, subj_end = _samp['subj_start'], _samp['subj_end']\n",
    "        obj_start, obj_end = _samp['obj_start'], _samp['obj_end']\n",
    "        _t[subj_start: subj_end + 1] = ['SUBJ-' +\n",
    "                                        _samp['subj_type']] * (subj_end - subj_start + 1)\n",
    "        _t[obj_start: obj_end + 1] = ['OBJ-' +\n",
    "                                      _samp['obj_type']] * (obj_end - obj_start + 1)\n",
    "        return _t\n",
    "\n",
    "    data = []\n",
    "    for path in ['./data/original/train.json',\n",
    "                 './data/original/dev.json',\n",
    "                 './data/original/test.json']:\n",
    "        with open(path, 'r', encoding='utf8') as f:\n",
    "            _obj = json.load(f)\n",
    "\n",
    "        data.append([{'token': entity_masks(sample),\n",
    "                      'relation': rel2id[sample['relation']],\n",
    "                      'subj_start': sample['subj_start'],\n",
    "                      'subj_end': sample['subj_end'],\n",
    "                      'obj_start': sample['obj_start'],\n",
    "                      'obj_end': sample['obj_end'],\n",
    "                      'pos': [pos2id[_p] for _p in sample['stanford_pos']],\n",
    "                      'ner': [ner2id[_n] for _n in sample['stanford_ner']]} for sample in _obj])\n",
    "\n",
    "    words = counter([samp['token'] for samp in data[0]], threshold=1)\n",
    "    word2id = {'pad': 0, 'unk': 1}\n",
    "    idx = 2\n",
    "    for w in words:\n",
    "        if w not in word2id:\n",
    "            word2id[w] = idx\n",
    "            idx += 1\n",
    "\n",
    "    weight_matrix = load_glove_emb(glove_path='./data/glove.6B.300d.txt',\n",
    "                                   embed_dim=300,\n",
    "                                   word2id=word2id)\n",
    "    np.save('./data/weight_matrix.npy', weight_matrix)\n",
    "\n",
    "    with open('./data/vocab.json', 'w') as f:\n",
    "        json.dump(word2id, f)\n",
    "\n",
    "    for _obj, path in zip(data, ['./data/train.json',\n",
    "                                 './data/dev.json',\n",
    "                                 './data/test.json']):\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(_obj, f)\n",
    "\n",
    "\n",
    "def load_glove_emb(glove_path, embed_dim, word2id):\n",
    "    pre_trained_weights = np.random.randn(len(word2id), embed_dim).astype(\n",
    "        np.float32) * np.sqrt(2.0 / len(word2id))\n",
    "    with open(glove_path, 'r', encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            line = line.split(' ')\n",
    "            word = line[0]\n",
    "            embedding_vector = np.array(line[1:]).astype(np.float32)\n",
    "            if word in word2id:\n",
    "                pre_trained_weights[word2id[word]] = embedding_vector\n",
    "\n",
    "    return pre_trained_weights\n",
    "\n",
    "# preprocess()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8655e2-aa2a-479f-8f01-e6a943220840",
   "metadata": {},
   "source": [
    "## TacRedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a45a8d-0e34-4693-b8c4-8fc61972332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TacRedDataset(Data.Dataset):\n",
    "    def __init__(self, data, word2id):\n",
    "        super(TacRedDataset, self).__init__()\n",
    "\n",
    "        self.sent_tensors_list, self.rel_list = [], []\n",
    "        self.subj_start_list, self.subj_end_list = [], []\n",
    "        self.obj_start_list, self.obj_end_list = [], []\n",
    "        self.pos_tensors_list, self.ner_tensors_list = [], []\n",
    "\n",
    "        for sample in data:\n",
    "            tokens = sample['token']\n",
    "            sent = torch.zeros(len(tokens), dtype=torch.long).to(device)\n",
    "            for t_idx, token in enumerate(tokens):\n",
    "                try:\n",
    "                    sent[t_idx] = word2id[token]\n",
    "                except KeyError:\n",
    "                    sent[t_idx] = word2id['unk']\n",
    "            self.sent_tensors_list.append(sent)\n",
    "\n",
    "            self.rel_list.append(int(sample['relation']))\n",
    "\n",
    "            self.subj_start_list.append(int(sample['subj_start']))\n",
    "            self.subj_end_list.append(int(sample['subj_end']))\n",
    "            self.obj_start_list.append(int(sample['obj_start']))\n",
    "            self.obj_end_list.append(int(sample['obj_end']))\n",
    "\n",
    "            self.pos_tensors_list.append(torch.tensor(sample['pos'], dtype=torch.long).to(device))\n",
    "\n",
    "            self.ner_tensors_list.append(torch.tensor(sample['ner'], dtype=torch.long).to(device))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rel_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sent = self.sent_tensors_list[index]\n",
    "        rel = self.rel_list[index]\n",
    "        subj_start = self.subj_start_list[index]\n",
    "        subj_end = self.subj_end_list[index]\n",
    "        obj_start = self.obj_start_list[index]\n",
    "        obj_end = self.obj_end_list[index]\n",
    "        pos = self.pos_tensors_list[index]\n",
    "        ner = self.ner_tensors_list[index]\n",
    "\n",
    "        return sent, rel, subj_start, subj_end, obj_start, obj_end, pos, ner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53930e0c-792b-4db0-862a-f0587faa83af",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bacbf3a-1423-438c-8937-81e534669a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(dataset: Data.Dataset, batch_size: int, shuffle: bool):\n",
    "    def collate_fn(batch):\n",
    "        sent, rel, subj_start, subj_end, obj_start, obj_end, pos, ner = zip(*batch)\n",
    "        _len = [len(s) for s in sent]\n",
    "\n",
    "        # sentence tensor\n",
    "        sent = pad_sequence(sent, batch_first=True)\n",
    "\n",
    "        # relation tensor\n",
    "        rel = torch.tensor(rel, dtype=torch.long).to(device)\n",
    "\n",
    "        _max_len = max(_len)\n",
    "\n",
    "        # subject position encoding\n",
    "        subj_pos = torch.zeros((len(batch), _max_len), dtype=torch.long).to(device)\n",
    "        for batch_idx, (_start, _end) in enumerate(zip(subj_start, subj_end)):\n",
    "            subj_pos[batch_idx] = get_positions(_start, _end, _max_len)\n",
    "\n",
    "        # object position encoding\n",
    "        obj_pos = torch.zeros((len(batch), _max_len), dtype=torch.long).to(device)\n",
    "        for batch_idx, (_start, _end) in enumerate(zip(obj_start, obj_end)):\n",
    "            obj_pos[batch_idx] = get_positions(_start, _end, _max_len)\n",
    "\n",
    "        pos = pad_sequence(pos, batch_first=True)\n",
    "\n",
    "        ner = pad_sequence(ner, batch_first=True)\n",
    "\n",
    "        # must be on the CPU if provided as a tensor\n",
    "        tensor_len = torch.tensor(_len, dtype=torch.long)\n",
    "\n",
    "        return sent, rel, subj_pos, obj_pos, pos, ner, tensor_len\n",
    "\n",
    "    data_loader = Data.DataLoader(dataset=dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  collate_fn=collate_fn,\n",
    "                                  shuffle=shuffle)\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "def get_positions(start_idx, end_idx, length):\n",
    "    \"\"\" Get subj/obj position encoding \"\"\"\n",
    "    position_enc = list(range(-start_idx, 0)) + \\\n",
    "        [0]*(end_idx - start_idx + 1) + list(range(1, length - end_idx))\n",
    "    return torch.tensor(position_enc, dtype=torch.long).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e0bd65-ad1d-4f1b-8d4f-a979a21df1a7",
   "metadata": {},
   "source": [
    "# early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3da906-321e-4b54-bfff-2a8672d0fc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping(object):\n",
    "    def __init__(self, mode='min', min_delta=0.001, patience=5, percentage=False):\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = 0\n",
    "        self.is_better = None\n",
    "        self._init_is_better(mode, min_delta, percentage)\n",
    "\n",
    "        if patience == 0:\n",
    "            self.is_better = lambda a, b: True\n",
    "            self.step = lambda a: False\n",
    "\n",
    "    def step(self, metrics):\n",
    "        if self.best is None:\n",
    "            self.best = metrics\n",
    "            return False\n",
    "        if math.isnan(metrics):\n",
    "            return True\n",
    "        if self.is_better(metrics, self.best):\n",
    "            self.num_bad_epochs = 0\n",
    "            self.best = metrics\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _init_is_better(self, mode, min_delta, percentage):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError('illegal mode: ', mode)\n",
    "        if not percentage:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - min_delta\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + min_delta\n",
    "        else:\n",
    "            if mode == 'min':\n",
    "                self.is_better = lambda a, best: a < best - (best * min_delta / 100)\n",
    "            if mode == 'max':\n",
    "                self.is_better = lambda a, best: a > best + (best * min_delta / 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c820e1b-36bd-439d-b026-c021dc8fd4cd",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f26db7-d752-4a69-a54d-2656dd899615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(experiment_time, model, optimizer):\n",
    "    mkdir(f'{root_dir}/results/checkpoints')\n",
    "    checkpoint_path = f'{root_dir}/results/checkpoints/' + experiment_time + '.pth'\n",
    "    checkpoint = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict}\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "\n",
    "def load_model(latest, file_name=None):\n",
    "    \"\"\" load the latest checkpoint \"\"\"\n",
    "    checkpoints_dir = f'{root_dir}/results/checkpoints'\n",
    "    if latest:\n",
    "        file_list = os.listdir(checkpoints_dir)\n",
    "        file_list.sort(key=lambda fn: os.path.getmtime(\n",
    "            checkpoints_dir + '/' + fn))\n",
    "        checkpoint = torch.load(checkpoints_dir + '/' + file_list[-1])\n",
    "        return checkpoint, str(file_list[-1])\n",
    "    else:\n",
    "        if file_name is None:\n",
    "            raise ValueError('checkpoint_path cannot be empty!')\n",
    "        checkpoint = torch.load(checkpoints_dir + '/' + file_name)\n",
    "        return checkpoint, file_name\n",
    "\n",
    "\n",
    "def weights_init(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.xavier_normal_(param.data, gain=1.0)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "\n",
    "def fix_seed(seed):\n",
    "    \"\"\" fix seed to ensure reproducibility \"\"\"\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def get_logger(filename, verbosity=1, name=None):\n",
    "    level_dict = {0: logging.DEBUG, 1: logging.INFO, 2: logging.WARNING}\n",
    "    formatter = logging.Formatter(\" %(message)s\")\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level_dict[verbosity])\n",
    "    fh = logging.FileHandler(filename, \"a\")\n",
    "    fh.setFormatter(formatter)\n",
    "    logger.addHandler(fh)\n",
    "    sh = logging.StreamHandler()\n",
    "    sh.setFormatter(formatter)\n",
    "    logger.addHandler(sh)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def record_time(start_time, end_time):\n",
    "    \"\"\" get minute & second-level measurement of the asc-time \"\"\"\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_min = int(elapsed_time / 60)\n",
    "    elapsed_sec = int(elapsed_time - (elapsed_min * 60))\n",
    "    return elapsed_min, elapsed_sec\n",
    "\n",
    "\n",
    "def mkdir(dir_path):\n",
    "    \"\"\" create folder if not exists. \"\"\"\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.mkdir(dir_path)\n",
    "\n",
    "\n",
    "def json_load(paths):\n",
    "    \"\"\" load json data. \"\"\"\n",
    "    data = []\n",
    "    for path in paths:\n",
    "        with open(path, 'r', encoding='utf8') as f:\n",
    "            data.append(json.load(f))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eef8711-bb52-433c-8dc3-fdff776f466d",
   "metadata": {},
   "source": [
    "## eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e62d0b-cfa2-4988-939f-19bf8ab119b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(key, prediction):\n",
    "    correct_by_relation = Counter()\n",
    "    guessed_by_relation = Counter()\n",
    "    gold_by_relation = Counter()\n",
    "    \n",
    "    NO_RELATION = rel2id['no_relation']\n",
    "\n",
    "    # Loop over the data to compute a score\n",
    "    for row in range(len(key)):\n",
    "        gold = key[row]\n",
    "        guess = prediction[row]\n",
    "\n",
    "        if gold == NO_RELATION and guess == NO_RELATION:\n",
    "            pass\n",
    "        elif gold == NO_RELATION and guess != NO_RELATION:\n",
    "            guessed_by_relation[guess] += 1\n",
    "        elif gold != NO_RELATION and guess == NO_RELATION:\n",
    "            gold_by_relation[gold] += 1\n",
    "        elif gold != NO_RELATION and guess != NO_RELATION:\n",
    "            guessed_by_relation[guess] += 1\n",
    "            gold_by_relation[gold] += 1\n",
    "            if gold == guess:\n",
    "                correct_by_relation[guess] += 1\n",
    "\n",
    "    # verbose information\n",
    "    relations = gold_by_relation.keys()\n",
    "    verbose_scores = {}\n",
    "    for relation in sorted(relations):\n",
    "        correct = correct_by_relation[relation]\n",
    "        guessed = guessed_by_relation[relation]\n",
    "        gold = gold_by_relation[relation]\n",
    "        prec = 1.0\n",
    "        if guessed > 0:\n",
    "            prec = float(correct) / float(guessed)\n",
    "        recall = 0.0\n",
    "        if gold > 0:\n",
    "            recall = float(correct) / float(gold)\n",
    "        f1 = 0.0\n",
    "        if prec + recall > 0:\n",
    "            f1 = 2.0 * prec * recall / (prec + recall)\n",
    "        verbose_scores[id2rel[relation]] = {'p': prec, 'r': recall, 'f1': f1}\n",
    "\n",
    "    # aggregate score\n",
    "    prec_micro = 1.0\n",
    "    if sum(guessed_by_relation.values()) > 0:\n",
    "        prec_micro = float(sum(correct_by_relation.values())) / \\\n",
    "            float(sum(guessed_by_relation.values()))\n",
    "    recall_micro = 0.0\n",
    "    if sum(gold_by_relation.values()) > 0:\n",
    "        recall_micro = float(sum(correct_by_relation.values())) / \\\n",
    "            float(sum(gold_by_relation.values()))\n",
    "    f1_micro = 0.0\n",
    "    if prec_micro + recall_micro > 0.0:\n",
    "        f1_micro = 2.0 * prec_micro * \\\n",
    "            recall_micro / (prec_micro + recall_micro)\n",
    "\n",
    "    return prec_micro * 100, recall_micro * 100, f1_micro * 100, verbose_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd65bd8-df4a-4bda-9654-58fdfe24a0ed",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc0fac0-5dd2-46e0-91e4-fa9154739fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationModel(nn.Module):\n",
    "    def __init__(self, use_glove, word_size, word_embed_dim, pos_size, pos_embed_dim,\n",
    "                 ner_size, ner_embed_dim, max_len, position_embed_dim,\n",
    "                 hidden_dim, rel_size, attn_dim, dropout, ) -> None:\n",
    "        super(RelationModel, self).__init__()\n",
    "        self.embed_dim = word_embed_dim + pos_embed_dim + ner_embed_dim\n",
    "        self.max_len = max_len\n",
    "\n",
    "        if use_glove:\n",
    "            self.word_embedding = self.load_glove()\n",
    "        else:\n",
    "            self.word_embedding = nn.Embedding(word_size, word_embed_dim, padding_idx=0)\n",
    "\n",
    "        self.pos_embedding = nn.Embedding(pos_size, pos_embed_dim, padding_idx=0)\n",
    "\n",
    "        self.ner_embedding = nn.Embedding(ner_size, ner_embed_dim, padding_idx=0)\n",
    "\n",
    "        self.position_embedding = nn.Embedding(max_len * 2 + 1, position_embed_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(self.embed_dim, hidden_dim, batch_first=True, bidirectional=False)\n",
    "\n",
    "        self.weight_reply = nn.Parameter(torch.randn(hidden_dim, hidden_dim), requires_grad=True)\n",
    "        self.fc_gate = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.layernorm_gate = nn.LayerNorm(hidden_dim, elementwise_affine=True)\n",
    "\n",
    "        self.fc_q = nn.Linear(hidden_dim, attn_dim)\n",
    "        self.fc_h = nn.Linear(hidden_dim, attn_dim)\n",
    "        self.fc_k = nn.Linear(position_embed_dim * 2, attn_dim)\n",
    "        self.fc_v = nn.Linear(attn_dim, 1)\n",
    "\n",
    "        self.fc_out = nn.Linear(hidden_dim, rel_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, seq_input: Tensor, pos: Tensor, ner: Tensor, subj_pos: Tensor, obj_pos: Tensor, tensor_len: Tensor, ) -> Tensor:\n",
    "        \"\"\"\n",
    "        # pos: part-of-speech, position: relative position\n",
    "        seq_input, pos, ner, subj_pos, obj_pos: (batch, seq_len)\n",
    "        \"\"\"\n",
    "        word_embed = self.word_embedding(seq_input)\n",
    "        pos_embed = self.pos_embedding(pos)\n",
    "        ner_embed = self.ner_embedding(ner)\n",
    "\n",
    "        encoder_embed = self.dropout(\n",
    "            torch.cat((word_embed, pos_embed, ner_embed), dim=-1))\n",
    "\n",
    "        packed = pack_padded_sequence(encoder_embed, lengths=tensor_len, batch_first=True, enforce_sorted=False)\n",
    "        lstm_output, (h_n, _) = self.lstm(packed)\n",
    "        lstm_output, _ = pad_packed_sequence(lstm_output, batch_first=True)\n",
    "        h_n = self.dropout(h_n.squeeze(0))\n",
    "        lstm_output = self.dropout(lstm_output)\n",
    "\n",
    "        subj_pos_embed = self.position_embedding(subj_pos + self.max_len)\n",
    "        obj_pos_embed = self.position_embedding(obj_pos + self.max_len)\n",
    "        position_embed = torch.cat((subj_pos_embed, obj_pos_embed), dim=-1)\n",
    "\n",
    "        entity_selection = self.entity_selection_gate(lstm_output)\n",
    "\n",
    "        output = self.weighted_rep(entity_selection, tensor_len, h_n, position_embed)\n",
    "\n",
    "        output = F.log_softmax(self.fc_out(output), dim=-1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def entity_selection_gate(self, reply_pre: Tensor) -> Tensor:\n",
    "        # reply_pre: (batch, seq_len, hidden_dim)\n",
    "        reply_post = reply_pre.permute(0, 2, 1)\n",
    "\n",
    "        alpha = torch.matmul(\n",
    "            torch.matmul(reply_pre, self.weight_reply), reply_post)\n",
    "        # mask padding\n",
    "        alpha = torch.softmax(alpha, dim=-1)\n",
    "        # weighted average\n",
    "        reply_c = torch.bmm(alpha, reply_pre)\n",
    "        attn_gate = torch.sigmoid(self.layernorm_gate(\n",
    "            self.fc_gate(torch.cat((reply_pre, reply_c), dim=2))))\n",
    "\n",
    "        reply_cs = attn_gate * reply_pre\n",
    "\n",
    "        return reply_cs\n",
    "\n",
    "    def weighted_rep(self, hidden_states: Tensor, tensor_len: Tensor, h_n: Tensor, pe_features: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        hidden_states: (batch, seq_len, hidden_dim)\n",
    "        tensor_len: (batch)\n",
    "        h_n: (batch, hidden_dim)\n",
    "        pe_features: (batch, seq_len, position_embed_dim * 2)\n",
    "        \"\"\"\n",
    "        _q = self.fc_q(hidden_states)\n",
    "        _h = self.fc_h(h_n).unsqueeze(1).expand_as(_q)\n",
    "        _k = self.fc_k(pe_features)\n",
    "        attn_score = self.fc_v(torch.tanh(sum([_q, _h, _k]))).squeeze(-1)\n",
    "\n",
    "        # mask padding\n",
    "        attn_score.data.masked_fill_(\n",
    "            self.build_mask(tensor_len.data), -float('inf'))\n",
    "        attn_weights = torch.softmax(attn_score, dim=-1)\n",
    "        # weighted average input vectors\n",
    "        output = attn_weights.unsqueeze(dim=1).bmm(hidden_states).squeeze(dim=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def load_glove(glove_path=f'{root_dir}/data/weight_matrix.npy'):\n",
    "        weight = torch.from_numpy(np.load(glove_path)).float()\n",
    "        pre_trained_embedding = nn.Embedding.from_pretrained(weight)\n",
    "        pre_trained_embedding.weight.requires_grad = True\n",
    "\n",
    "        return pre_trained_embedding\n",
    "\n",
    "    @staticmethod\n",
    "    def build_mask(x: Tensor) -> Tensor:\n",
    "        # x: (batch)\n",
    "        z = torch.zeros((len(x), int(max(x))), dtype=torch.long).to(device)\n",
    "        for _x, _z in zip(x, z):\n",
    "            _z[_x:] = 1\n",
    "        return z.data.to(bool)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8304e89-0960-4bf2-85bc-1aebc8c85f01",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808b8d7e-1593-42b5-8816-232050f8744f",
   "metadata": {},
   "source": [
    "## Load DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf1500b-d2c1-4a40-8c38-14261f716bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seed(args.random_seed)\n",
    "\n",
    "cur_time = datetime.utcnow().replace(tzinfo=timezone.utc).astimezone(\n",
    "    timezone(timedelta(hours=8))).strftime('%Y_%m_%d_%H_%M')\n",
    "logger = get_logger(f'{root_dir}/results/logs/' + cur_time + '.log')\n",
    "\n",
    "# Print arguments\n",
    "for arg in vars(args):\n",
    "    logger.info(\"{} = {}\".format(arg, getattr(args, arg)))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load DataLoader\n",
    "train_data, dev_data, test_data, word2id = json_load([f'{root_dir}/data/debug/train.json',\n",
    "                                                      f'{root_dir}/data/debug/dev.json',\n",
    "                                                      f'{root_dir}/data/debug/test.json', \n",
    "                                                      f'{root_dir}/data/vocab.json'])\n",
    "train_data_loader = get_data_loader(dataset=TacRedDataset(data=train_data, word2id=word2id),\n",
    "                                    batch_size=args.batch_size,\n",
    "                                    shuffle=True)\n",
    "dev_data_loader = get_data_loader(dataset=TacRedDataset(data=dev_data, word2id=word2id),\n",
    "                                  batch_size=1,\n",
    "                                  shuffle=False)\n",
    "test_data_loader = get_data_loader(dataset=TacRedDataset(data=test_data, word2id=word2id),\n",
    "                                   batch_size=1,\n",
    "                                   shuffle=False)\n",
    "logger.info(f'data processing consumes: {(time.time() - start_time):.2f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8314debe-fedf-475f-8a33-effe1e3c772e",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6f6f6e-82da-42e9-bad6-baf3a6cecbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RelationModel(use_glove=args.use_glove,\n",
    "                      word_size=len(word2id),\n",
    "                      word_embed_dim=args.word_embed_dim,\n",
    "                      pos_size=len(pos2id),\n",
    "                      pos_embed_dim=args.pos_embed_dim,\n",
    "                      ner_size=len(ner2id),\n",
    "                      ner_embed_dim=args.ner_embed_dim,\n",
    "                      max_len=max_len,\n",
    "                      position_embed_dim=args.position_embed_dim,\n",
    "                      hidden_dim=args.hidden_dim,\n",
    "                      rel_size=len(rel2id),\n",
    "                      attn_dim=args.attn_dim,\n",
    "                      dropout=args.dropout).to(device)\n",
    "model.apply(weights_init)\n",
    "\n",
    "# Optimizer\n",
    "if args.optimizer == 'sgd':\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "elif args.optimizer == 'adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "elif args.optimizer == 'adadelta':\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "else:\n",
    "    raise ValueError('Choose Optimizer from SGD/ Adam/ Adadelta. !')\n",
    "\n",
    "# Cross Entropy Loss\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "early_stop = EarlyStopping(mode='max', min_delta=0.0001, patience=5)\n",
    "\n",
    "# Load Checkpoint\n",
    "if args.resume:\n",
    "    checkpoint, cp_name = load_model(latest=True)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    logger.info(f'load checkpoint: [{cp_name}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530527cd-97b7-4c7a-ada1-30deec19b21e",
   "metadata": {},
   "source": [
    "## Functions for train/ dev/ test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0c541c-e6a7-45c6-89f1-fd00249df3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for sent, rel, subj_pos, obj_pos, pos, ner, tensor_len in tqdm(train_data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(sent, pos, ner, subj_pos, obj_pos, tensor_len)\n",
    "        loss = criterion(output, rel)\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
    "        optimizer.step()\n",
    "    return epoch_loss / len(train_data_loader)\n",
    "\n",
    "\n",
    "def evaluate(eval_data_loader):\n",
    "    model.eval()\n",
    "    gold_labels, pred_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for sent, rel, subj_pos, obj_pos, pos, ner, tensor_len in eval_data_loader:\n",
    "            output = model(sent, pos, ner, subj_pos, obj_pos, tensor_len)\n",
    "            rel_pred = torch.argmax(output, dim=1)\n",
    "            pred_labels.append(rel_pred.cpu().item())\n",
    "            gold_labels.append(rel.cpu().item())\n",
    "\n",
    "    return compute_score(key=gold_labels, prediction=pred_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53e8bfe-87fc-4cd9-bfb3-7a5fc7c99a14",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e155856-f1dc-417a-a0a6-7df9b2747b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1 = 0\n",
    "for epoch in range(1, int(args.max_epochs + 1)):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train()\n",
    "    _, _, dev_f1, _ = evaluate(dev_data_loader)\n",
    "    epoch_min, epoch_sec = record_time(start_time, time.time())\n",
    "\n",
    "    logger.info(\n",
    "        f'epoch: [{epoch:02}/{args.max_epochs}]  train_loss={train_loss:.3f}  '\n",
    "        f'dev_f1={dev_f1:.2f}  duration: {epoch_min}m {epoch_sec}s')\n",
    "\n",
    "    if dev_f1 > best_f1:\n",
    "        save_model(experiment_time=cur_time, model=model, optimizer=optimizer)\n",
    "\n",
    "    if early_stop.step(dev_f1):\n",
    "        logger.info(f'early stop at [{epoch:02}/{args.max_epochs}]')\n",
    "        break\n",
    "\n",
    "test_prec, test_recall, test_f1, verbose_info = evaluate(test_data_loader)\n",
    "\n",
    "logger.info(\n",
    "    f'precision: {test_prec:.2f}  recall: {test_recall:.2f}  f1: {test_f1:.2f}')\n",
    "logger.info(verbose_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52f4301-1aab-4b89-9530-4b43ff5857c6",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45414d3-f3c7-4839-aeb9-838a52a93dac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
